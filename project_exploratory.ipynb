{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 607.  Tensorflow Project.  Exploratory Data Analysis\n",
    "## May 10, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gamma\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods from the Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_json(typename, api, body):\n",
    "    \"\"\"\n",
    "    read_data_json directly accesses the C3.ai COVID-19 Data Lake APIs using the requests library, \n",
    "    and returns the response as a JSON, raising an error if the call fails for any reason.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    api: The API you want to access, either 'fetch' or 'evalmetrics'.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.c3.ai/covid/api/1/\" + typename + \"/\" + api, \n",
    "        json = body, \n",
    "        headers = {\n",
    "            'Accept' : 'application/json', \n",
    "            'Content-Type' : 'application/json'\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def fetch(typename, body, get_all = False, remove_meta = True):\n",
    "    \"\"\"\n",
    "    fetch accesses the Data Lake using read_data_json, and converts the response into a Pandas dataframe. \n",
    "    fetch is used for all non-timeseries data in the Data Lake, and will call read_data as many times \n",
    "    as required to access all of the relevant data for a given typename and body.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    get_all: If True, get all records and ignore any limit argument passed in the body. If False, use the limit argument passed in the body. The default is False.\n",
    "    remove_meta: If True, remove metadata about each record. If False, include it. The default is True.\n",
    "    \"\"\"\n",
    "    if get_all:\n",
    "        has_more = True\n",
    "        offset = 0\n",
    "        limit = 2000\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        while has_more:\n",
    "            body['spec'].update(limit = limit, offset = offset)\n",
    "            response_json = read_data_json(typename, 'fetch', body)\n",
    "            new_df = pd.json_normalize(response_json['objs'])\n",
    "            df = df.append(new_df)\n",
    "            has_more = response_json['hasMore']\n",
    "            offset += limit\n",
    "            \n",
    "    else:\n",
    "        response_json = read_data_json(typename, 'fetch', body)\n",
    "        df = pd.json_normalize(response_json['objs'])\n",
    "        \n",
    "    if remove_meta:\n",
    "        df = df.drop(columns = [c for c in df.columns if ('meta' in c) | ('version' in c)])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def evalmetrics(typename, body, remove_meta = True):\n",
    "    \"\"\"\n",
    "    evalmetrics accesses the Data Lake using read_data_json, and converts the response into a Pandas dataframe.\n",
    "    evalmetrics is used for all timeseries data in the Data Lake.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    remove_meta: If True, remove metadata about each record. If False, include it. The default is True.\n",
    "    \"\"\"\n",
    "    response_json = read_data_json(typename, 'evalmetrics', body)\n",
    "    df = pd.json_normalize(response_json['result'])\n",
    "    \n",
    "    # get the useful data out\n",
    "    df = df.apply(pd.Series.explode)\n",
    "    if remove_meta:\n",
    "        df = df.filter(regex = 'dates|data|missing')\n",
    "    \n",
    "    # only keep one date column\n",
    "    date_cols = [col for col in df.columns if 'dates' in col]\n",
    "    keep_cols =  date_cols[:1] + [col for col in df.columns if 'dates' not in col]\n",
    "    df = df.filter(items = keep_cols).rename(columns = {date_cols[0] : \"dates\"})\n",
    "    df[\"dates\"] = pd.to_datetime(df[\"dates\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streamlined request for single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_one(typename: str, body: dict, objs_only=True) -> dict:\n",
    "    \"\"\"\n",
    "    Returns JSON output from single API call\n",
    "    \n",
    "    Args:\n",
    "        typename: the C3.ai type name\n",
    "        body: the body of the request\n",
    "        objs_only: if True, remove the metadata and just returns the objects\n",
    "        \n",
    "    Returns:\n",
    "        JSON response as dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    response = read_data_json(typename, 'fetch', body)\n",
    "    if objs_only:\n",
    "        for r in response['objs']:\n",
    "            if 'meta' in r.keys():\n",
    "                del(r['meta'])\n",
    "                \n",
    "        return response['objs']\n",
    "    \n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the location codes for the US into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_locations(file_name='C3-ai-Location-IDs.xlsx'): \n",
    "    \"\"\" Loads all US counties from C3 ai spreadsheet \n",
    "    \n",
    "    Args:\n",
    "        file_name: the name of the spreadsheet\n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe with the results\n",
    "    \n",
    "    \"\"\"\n",
    "                     \n",
    "    locations = pd.read_excel(path.join('.', file_name), sheet_name='County IDs', header=2)\n",
    "    us_locations = locations[locations.Country=='United States']\n",
    "    \n",
    "    return us_locations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the basic population data for each of the 3429 counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outbreaklocation_body(county_id: str) -> dict:\n",
    "    \"\"\" Forms the request body for a count for the outbreak location API \n",
    "    \n",
    "    Args:\n",
    "        count_id: the ID for the County\n",
    "    \n",
    "    Returns:\n",
    "        The request body\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "              \"spec\": {\n",
    "                \"filter\": f\"id == '{county_id}'\"\n",
    "              }\n",
    "}\n",
    "\n",
    "# fetch_one('outbreaklocation', make_outbreaklocation_body('Autauga_Alabama_UnitedStates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_population_data(file_name='counties.json'):\n",
    "    \"\"\" Loads all population data for US counties and stores in a file called counties.json\"\"\"\n",
    "\n",
    "    us_locations = get_us_locations()\n",
    "    keep_going = True\n",
    "    tries = 0\n",
    "    while keep_going:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                county_data = json.load(file)\n",
    "        except:    \n",
    "            county_data = {}\n",
    "        i = 0\n",
    "        for county in us_locations['County id']:\n",
    "            if county not in county_data.keys():\n",
    "                try:\n",
    "                    data = fetch_one('outbreaklocation',  make_outbreaklocation_body(county))\n",
    "                    county_data[county] = data[0]\n",
    "                    i += 1\n",
    "                    if i % 100 == 0:\n",
    "                        print(f'Saving: {i}')\n",
    "                        with open(file_name, 'w') as file:\n",
    "                            json.dump(county_data, file)\n",
    "                except:\n",
    "                    county_data[county] = None\n",
    "                    print(f'Problem with {county}')\n",
    "                sleep(1)\n",
    "        with open('counties.json', 'w') as file:\n",
    "            json.dump(county_data, file)\n",
    "        if len(county_data) >= len(us_locations) or tries >= 5:\n",
    "            keep_going = False\n",
    "        else:\n",
    "            tries += 1\n",
    "        \n",
    "def get_counties_df(file_name='counties.json'):\n",
    "    with open(file_name) as file:\n",
    "                county_data = json.load(file)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(county_data)\n",
    "    \n",
    "    data = [df[col] for col in cols]    \n",
    "    \n",
    "    # pivot\n",
    "    return pd.DataFrame(data,columns=df.index, index=df.columns)\n",
    "    \n",
    "# get_counties_df()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the County Stats from the Census Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>PST045212</th>\n",
       "      <th>PST040210</th>\n",
       "      <th>PST120212</th>\n",
       "      <th>POP010210</th>\n",
       "      <th>AGE135212</th>\n",
       "      <th>AGE295212</th>\n",
       "      <th>AGE775212</th>\n",
       "      <th>SEX255212</th>\n",
       "      <th>RHI125212</th>\n",
       "      <th>...</th>\n",
       "      <th>SBO415207</th>\n",
       "      <th>SBO015207</th>\n",
       "      <th>MAN450207</th>\n",
       "      <th>WTN220207</th>\n",
       "      <th>RTN130207</th>\n",
       "      <th>RTN131207</th>\n",
       "      <th>AFN120207</th>\n",
       "      <th>BPS030212</th>\n",
       "      <th>LND110210</th>\n",
       "      <th>POP060210</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>313914040</td>\n",
       "      <td>308747508</td>\n",
       "      <td>1.7</td>\n",
       "      <td>308745538</td>\n",
       "      <td>6.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>13.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>77.9</td>\n",
       "      <td>...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>28.8</td>\n",
       "      <td>5319456312</td>\n",
       "      <td>4174286516</td>\n",
       "      <td>3917663456</td>\n",
       "      <td>12990</td>\n",
       "      <td>613795732</td>\n",
       "      <td>829658</td>\n",
       "      <td>3531905.43</td>\n",
       "      <td>87.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>4822023</td>\n",
       "      <td>4779745</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4779736</td>\n",
       "      <td>6.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>51.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>28.1</td>\n",
       "      <td>112858843</td>\n",
       "      <td>52252752</td>\n",
       "      <td>57344851</td>\n",
       "      <td>12364</td>\n",
       "      <td>6426342</td>\n",
       "      <td>13506</td>\n",
       "      <td>50645.33</td>\n",
       "      <td>94.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>55514</td>\n",
       "      <td>54571</td>\n",
       "      <td>1.7</td>\n",
       "      <td>54571</td>\n",
       "      <td>6.5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51.3</td>\n",
       "      <td>78.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>598175</td>\n",
       "      <td>12003</td>\n",
       "      <td>88157</td>\n",
       "      <td>385</td>\n",
       "      <td>594.44</td>\n",
       "      <td>91.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>190790</td>\n",
       "      <td>182265</td>\n",
       "      <td>4.7</td>\n",
       "      <td>182265</td>\n",
       "      <td>5.9</td>\n",
       "      <td>22.6</td>\n",
       "      <td>17.7</td>\n",
       "      <td>51.2</td>\n",
       "      <td>87.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>27.3</td>\n",
       "      <td>1410273</td>\n",
       "      <td>0</td>\n",
       "      <td>2966489</td>\n",
       "      <td>17166</td>\n",
       "      <td>436955</td>\n",
       "      <td>1184</td>\n",
       "      <td>1589.78</td>\n",
       "      <td>114.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>27201</td>\n",
       "      <td>27457</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>27457</td>\n",
       "      <td>5.6</td>\n",
       "      <td>21.2</td>\n",
       "      <td>15.2</td>\n",
       "      <td>46.3</td>\n",
       "      <td>50.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>188337</td>\n",
       "      <td>6334</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>884.88</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3190</th>\n",
       "      <td>56037</td>\n",
       "      <td>45267</td>\n",
       "      <td>43806</td>\n",
       "      <td>3.3</td>\n",
       "      <td>43806</td>\n",
       "      <td>7.9</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>47.8</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>27.2</td>\n",
       "      <td>0</td>\n",
       "      <td>437493</td>\n",
       "      <td>898189</td>\n",
       "      <td>22843</td>\n",
       "      <td>150439</td>\n",
       "      <td>132</td>\n",
       "      <td>10426.65</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3191</th>\n",
       "      <td>56039</td>\n",
       "      <td>21675</td>\n",
       "      <td>21294</td>\n",
       "      <td>1.8</td>\n",
       "      <td>21294</td>\n",
       "      <td>6.1</td>\n",
       "      <td>19.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>47.7</td>\n",
       "      <td>95.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>515644</td>\n",
       "      <td>25688</td>\n",
       "      <td>327363</td>\n",
       "      <td>122</td>\n",
       "      <td>3995.38</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3192</th>\n",
       "      <td>56041</td>\n",
       "      <td>21025</td>\n",
       "      <td>21118</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>21118</td>\n",
       "      <td>7.8</td>\n",
       "      <td>29.3</td>\n",
       "      <td>9.9</td>\n",
       "      <td>49.4</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0</td>\n",
       "      <td>159375</td>\n",
       "      <td>413983</td>\n",
       "      <td>20626</td>\n",
       "      <td>35497</td>\n",
       "      <td>38</td>\n",
       "      <td>2081.26</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>56043</td>\n",
       "      <td>8464</td>\n",
       "      <td>8533</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>8533</td>\n",
       "      <td>5.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>49.8</td>\n",
       "      <td>95.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.9</td>\n",
       "      <td>0</td>\n",
       "      <td>12128</td>\n",
       "      <td>98308</td>\n",
       "      <td>12596</td>\n",
       "      <td>10175</td>\n",
       "      <td>1</td>\n",
       "      <td>2238.55</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>56045</td>\n",
       "      <td>7082</td>\n",
       "      <td>7208</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>7208</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16.9</td>\n",
       "      <td>47.4</td>\n",
       "      <td>95.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0</td>\n",
       "      <td>11540</td>\n",
       "      <td>64312</td>\n",
       "      <td>9395</td>\n",
       "      <td>7520</td>\n",
       "      <td>0</td>\n",
       "      <td>2398.09</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3195 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fips  PST045212  PST040210  PST120212  POP010210  AGE135212  AGE295212  \\\n",
       "0         0  313914040  308747508        1.7  308745538        6.4       23.5   \n",
       "1      1000    4822023    4779745        0.9    4779736        6.3       23.3   \n",
       "2      1001      55514      54571        1.7      54571        6.5       26.0   \n",
       "3      1003     190790     182265        4.7     182265        5.9       22.6   \n",
       "4      1005      27201      27457       -0.9      27457        5.6       21.2   \n",
       "...     ...        ...        ...        ...        ...        ...        ...   \n",
       "3190  56037      45267      43806        3.3      43806        7.9       27.0   \n",
       "3191  56039      21675      21294        1.8      21294        6.1       19.4   \n",
       "3192  56041      21025      21118       -0.4      21118        7.8       29.3   \n",
       "3193  56043       8464       8533       -0.8       8533        5.9       25.0   \n",
       "3194  56045       7082       7208       -1.7       7208        6.0       21.9   \n",
       "\n",
       "      AGE775212  SEX255212  RHI125212  ...  SBO415207  SBO015207   MAN450207  \\\n",
       "0          13.7       50.8       77.9  ...        8.3       28.8  5319456312   \n",
       "1          14.5       51.5       70.0  ...        1.2       28.1   112858843   \n",
       "2          13.0       51.3       78.5  ...        0.7       31.7           0   \n",
       "3          17.7       51.2       87.3  ...        1.3       27.3     1410273   \n",
       "4          15.2       46.3       50.5  ...        0.0       27.0           0   \n",
       "...         ...        ...        ...  ...        ...        ...         ...   \n",
       "3190        8.8       47.8       94.5  ...        3.8       27.2           0   \n",
       "3191       11.2       47.7       95.3  ...        3.3       25.3           0   \n",
       "3192        9.9       49.4       96.0  ...        2.2       15.9           0   \n",
       "3193       18.7       49.8       95.3  ...        0.0       26.9           0   \n",
       "3194       16.9       47.4       95.4  ...        0.0       29.3           0   \n",
       "\n",
       "       WTN220207   RTN130207  RTN131207  AFN120207  BPS030212   LND110210  \\\n",
       "0     4174286516  3917663456      12990  613795732     829658  3531905.43   \n",
       "1       52252752    57344851      12364    6426342      13506    50645.33   \n",
       "2              0      598175      12003      88157        385      594.44   \n",
       "3              0     2966489      17166     436955       1184     1589.78   \n",
       "4              0      188337       6334          0          2      884.88   \n",
       "...          ...         ...        ...        ...        ...         ...   \n",
       "3190      437493      898189      22843     150439        132    10426.65   \n",
       "3191           0      515644      25688     327363        122     3995.38   \n",
       "3192      159375      413983      20626      35497         38     2081.26   \n",
       "3193       12128       98308      12596      10175          1     2238.55   \n",
       "3194       11540       64312       9395       7520          0     2398.09   \n",
       "\n",
       "      POP060210  \n",
       "0          87.4  \n",
       "1          94.4  \n",
       "2          91.8  \n",
       "3         114.6  \n",
       "4          31.0  \n",
       "...         ...  \n",
       "3190        4.2  \n",
       "3191        5.3  \n",
       "3192       10.1  \n",
       "3193        3.8  \n",
       "3194        3.0  \n",
       "\n",
       "[3195 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_county_stats_df(file_name='county_stats.csv'):\n",
    "    return pd.read_csv(path.join('.',file_name))\n",
    "\n",
    "get_county_stats_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Download the Evalmetrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_last_file_date(county: str) -> str:\n",
    "    max_date = '2020-01-01'\n",
    "    files = glob(path.join('.', 'data', f'{county}*.psv'))\n",
    "    if not files:\n",
    "        return max_date\n",
    "    for file in files:\n",
    "        match = re.search(r'(\\d\\d\\d\\d-\\d\\d-\\d\\d).psv', file)\n",
    "        if match:\n",
    "            max_date = max(max_date, match.group(1))\n",
    "    return max_date\n",
    "            \n",
    "    \n",
    "def download_evalmetrics_data():\n",
    "    \"\"\" Downloads the evalmetrics data from the last download through current \"\"\"\n",
    "    \n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Get the list of counties\n",
    "    counties_file = path.join('.', 'counties.json')\n",
    "    with open(counties_file) as file:\n",
    "        counties = json.load(file)\n",
    "\n",
    "    # Iterate through counties saving the time series data\n",
    "    for county, details in counties.items():\n",
    "        print(county)\n",
    "\n",
    "        # Skip if missing county details\n",
    "        if not details:\n",
    "            continue\n",
    "\n",
    "        # Get the last date we processed\n",
    "        last_date = get_last_file_date(county)\n",
    "\n",
    "        if last_date == today:\n",
    "            continue\n",
    "\n",
    "        # Get the data for the county from the last date processed\n",
    "        body = {\"spec\" : {\n",
    "                            \"ids\" : [county],\n",
    "                            \"expressions\": [ \"JHU_ConfirmedCases\", \"JHU_ConfirmedDeaths\", \"JHU_ConfirmedRecoveries\"], \n",
    "                            \"start\" : last_date,\n",
    "                            \"end\" : today,\n",
    "                            \"interval\" : \"DAY\",\n",
    "                        }\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            df = evalmetrics(\"outbreaklocation\", body)\n",
    "            file_name = path.join('.', 'data', f'{county}-{last_date}-{today}.psv')\n",
    "            df.to_csv(file_name, sep='|')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {county}: {e}')\n",
    "        \n",
    "        sleep(1)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the downloaded data and produce raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_dataframe(files):\n",
    "    name_pattern = re.compile('([\\w_]+)')\n",
    "    county_df = None\n",
    "\n",
    "    for file in files:\n",
    "        base = path.basename(file)\n",
    "        try:\n",
    "            county = name_pattern.match(base).group(1)\n",
    "            records = []\n",
    "            with open(file) as fd:\n",
    "                reader = csv.reader(fd, delimiter='|')\n",
    "                # advance past the header\n",
    "                next(iter(reader))\n",
    "                records = [[county, row[1], row[2], row[4], row[6]] for row in reader]\n",
    "            temp_df = pd.DataFrame(records, columns=['county', 'date', 'confirmed_cases', 'confirmed_deaths', 'confirmed_recoveries'])\n",
    "            if county_df is None:\n",
    "                county_df = temp_df\n",
    "            else:\n",
    "                county_df = pd.concat([county_df, temp_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f'Failure on file {base} with error {e}')\n",
    "        \n",
    "    return county_df\n",
    "\n",
    "def join_evalmetrics_to_county(evalmetrics_df, counties_df, county_stats_df):\n",
    "    df = evalmetrics_df.merge(counties_df, how='left', left_on='county', right_index=True, sort=True)\n",
    "    # df.fips = [int(fips['id']) for fips in df.fips]\n",
    "    fips = []\n",
    "    for f in df.fips:\n",
    "        if isinstance(f, dict):\n",
    "            fips.append(int(f['id']))\n",
    "        else:\n",
    "            fips.append(f)\n",
    "    df.fips = fips\n",
    "    df = df.merge(county_stats_df, how='left', left_on='fips', right_on='fips')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runners\n",
    "#### The following runners can take a significant amount of time to process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Evalmetrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_evalmetrics_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the raw dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalmetric_df = append_to_dataframe([path.join('.','data', file) for file in os.listdir(path.join('.', 'data'))])\n",
    "\n",
    "counties_df = get_counties_df()    \n",
    "county_stats_df = get_county_stats_df()\n",
    "\n",
    "df = join_evalmetrics_to_county(evalmetric_df, counties_df, county_stats_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### To be used for small testing\n",
    "# evalmetrics_df = append_to_dataframe([path.join('.','data','Wayne_Ohio_UnitedStates-2020-01-01-2020-05-14.psv'),\n",
    "#                      path.join('.','data','Faulkner_Arkansas_UnitedStates-2020-01-01-2020-05-14.psv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
